{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "environmental-census",
   "metadata": {},
   "source": [
    "# Tests d'estimation et p-valeurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-equity",
   "metadata": {},
   "source": [
    "## Estimation du maximum de vraisemblance, p-valeur globale\n",
    "\n",
    "Si on est sûr du modèle de probabilité en $\\cos^2(\\alpha-\\theta)$,\n",
    "mais qu'on ne connaît pas $\\theta$, on peut chercher la valeur de\n",
    "$\\theta$ qui donne le maximum de vraisemblance, qui peut se calculer\n",
    "en fonction des probabilités théoriques et empiriques pour l'ensemble\n",
    "des $\\alpha$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c82c842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pandas/core/arraylike.py:402: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_MLE=29.900000000000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # type: ignore\n",
    "import scipy.stats as sps\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# On va travailler sur le jeu de données '1photon-polar-30-intervalles'.\n",
    "# Remarquer que les angles de mesure sont réguliers, c'est ce qui va nous\n",
    "# permettre de calculer des valeurs-p ci-dessous. (Sinon il faudrait binner.)\n",
    "data = pd.read_csv('datasets/jour1-1-facile/1photon-polar-30-intervalles.csv', sep=';')\n",
    "obs = data.X\n",
    "table_alpha = data.alpha\n",
    "\n",
    "# Définir une fonction pour la loi de probabilité, en fonction des angles en degrés.\n",
    "# La fonction peut opérer sur des nombres ou des tableaux.\n",
    "def f_proba(alpha, theta):\n",
    "    return np.cos(np.radians(alpha - theta)) ** 2\n",
    "\n",
    "# Définir une fonction de vraisemblance. (Spécifique aux valeurs de table_alpha.)\n",
    "def L(theta):\n",
    "    table_probas = f_proba(table_alpha, theta)\n",
    "    return (np.sum(np.log(table_probas[obs == 1]))\n",
    "           + np.sum(np.log(1 - table_probas[obs == 0]))\n",
    "           )\n",
    "\n",
    "# Repérer l'angle ayant le maximum de vraisemblance à 0.1 degré près (overkill).\n",
    "test_theta = np.arange(0, 180, 0.1)\n",
    "indice_MLE = np.argmax(np.asarray([L(theta) for theta in test_theta]))\n",
    "theta_MLE = test_theta[indice_MLE]\n",
    "print(f'{theta_MLE=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dbb8b1",
   "metadata": {},
   "source": [
    "### p-valeur et intervalle de confiance\n",
    "\n",
    "Quand on fait l'hypothèse qu'une certaine valeur de $\\theta$ est correcte,\n",
    "alors pour chaque valeur de $\\alpha$, on peut calculer une p-valeur en\n",
    "comparant la probabilité empirique de détection pour cet $\\alpha$ à la loi\n",
    "en $\\cos^2(\\alpha-\\theta)$ ; et on peut ensuite combiner l'ensemble de ces\n",
    "p-valeurs en une seule, spécifique à la valeur testée de $\\theta$.\n",
    "\n",
    "Cette p-valeur est alors la probabilité que l'hypothèse sur la valeur de\n",
    "$\\theta$ soit correcte.  Si elle est trop faible (p. ex. inférieure à 0.05),\n",
    "on rejette l'hypothèse.  Ceci permet alors de calculer un intervalle de\n",
    "confiance qui sera l'ensemble des valeurs de $\\theta$ que l'on ne peut pas\n",
    "écarter, parce que la p-valeur est supérieure au seuil choisi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-google",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05780228 0.08048041 0.10539839 0.12055046 0.12171962 0.11701056\n",
      " 0.10699797 0.09291783 0.07645766 0.05511524]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1852548/1043220646.py:27: RuntimeWarning: divide by zero encountered in log\n",
      "  Tchi = -2 * sum([np.log(x) for x in pvalues])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[29.0, 29.900000000000002]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcul des valeurs-p pour chaque angle de mesure rencontré,\n",
    "# à partir du nombre de fois où on l'a rencontré et de la valeur\n",
    "# moyenne de l'observation en X.\n",
    "table_alpha_uniques = np.unique(table_alpha)\n",
    "nb_alpha = np.asarray([len(table_alpha[table_alpha == a]) for a in table_alpha_uniques])\n",
    "moyenne_obs_alpha = np.asarray([np.mean(obs[table_alpha == a]) for a in table_alpha_uniques])\n",
    "#t = sps.norm.ppf(0.05) # FIXME this was in the old lab, not used?\n",
    "\n",
    "# Le bloc suivant élimine les angles pour lesquels les probabilités\n",
    "# sont trop hautes ou trop basses.  Essayez sans pour voir ;\n",
    "# pourquoi posent-elles problème ?\n",
    "indices_alpha_ok = (moyenne_obs_alpha > 0.1) & (moyenne_obs_alpha < 0.9)\n",
    "table_alpha_uniques = table_alpha_uniques[indices_alpha_ok]\n",
    "moyenne_obs_alpha = moyenne_obs_alpha[indices_alpha_ok]\n",
    "nb_alpha = nb_alpha[indices_alpha_ok]\n",
    "\n",
    "# Fonction pour calculer la valeur-p pour un theta spécifique.\n",
    "def pvalue(theta):\n",
    "    # Probabilité attendue pour chaque angle.\n",
    "    p0 = f_proba(table_alpha_uniques, theta)\n",
    "\n",
    "    # Valeur-p pour chaque angle.\n",
    "    pvalues = [2*(1 - sps.norm.cdf(abs(p - x)*(n/(p*(1-p)))**0.5))\n",
    "               for p,x,n in zip(moyenne_obs_alpha, p0, nb_alpha)]\n",
    "\n",
    "    # Combinaison des valeurs-p par la méthode de Fisher.\n",
    "    Tchi = -2 * sum([np.log(x) for x in pvalues])\n",
    "\n",
    "    # Retrouver la valeur-p correspondante via la distribution chi²\n",
    "    # avec 2*(nombre de valeurs-p) degrés de liberté.\n",
    "    return 1 - sps.chi2.cdf(Tchi, 2*len(table_alpha_uniques))\n",
    "\n",
    "# Intervalle de confiance.\n",
    "table_pvalues = np.asarray([pvalue(theta) for theta in test_theta])\n",
    "print(table_pvalues[table_pvalues > 0.05])\n",
    "indices_pvaleur_grande = np.where(table_pvalues > 0.05)[0]\n",
    "CI = [test_theta[indices_pvaleur_grande[0]], test_theta[indices_pvaleur_grande[-1]]]\n",
    "CI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-turkish",
   "metadata": {},
   "source": [
    "## Inégalité CHSH, test d'intrication\n",
    "\n",
    "L'inégalité de Clauser-Horne-Shimony-Holt (CHSH) montre que, si l'expérience de mesure à 2 photons pouvait être modélisée par une théorie à variables cachées locales, on aurait : $|S| \\leqslant 2$, où :\n",
    "$$S = E(a1,b1) - E(a1,b2) + E(a2,b1) + E(a2,b2),$$\n",
    "avec $a1,b1$ et $a1,b2$ et $a2,b1$ et $a2,b2$ 4 combinaisons différentes des angles de mesure $\\alpha$ et $\\beta$,\n",
    "et :\n",
    "$$E(x,y) = P_{11}^{xy} - P_{10}^{xy} - P_{01}^{xy} + P_{00}^{xy}  $$\n",
    "avec $ P_{11}^{xy} = \\mathbb{P}(Xa = 1,Xb=1|\\alpha = x, \\beta = y)$, etc.\n",
    "\n",
    "On peut générer $\\hat{S}$ la valeur empirique de $S$. Soit $n$ le nombre de points de mesure pour chaque combinaison d'angles $\\alpha, \\beta$ (supposé le même pour toutes les combinaisons ; sinon voir plus bas). Soit $X_{i}^{ab} \\in \\{0,1\\}$ et $Y_{i}^{ab}\\in \\{0,1\\}$ le résultat de la $i$-ème mesure avec les angles $a,b$. On définit alors :\n",
    "$$I_i(a,b) = X_{i}^{ab} Y_{i}^{ab} + X_{i}^{ab} (1-Y_{i}^{ab}) - (1-X_{i}^{ab})Y_{i}^{ab} + (1-X_{i}^{ab})(1-X_{i}^{ab}).$$\n",
    "On peut alors écrire la variable aléatoire :\n",
    "$$Z_i = I(a1,b1) - I(a1,b2) + I(a2,b1) + I(a2,b2)$$\n",
    "où l'on voit que les $Z$ sont des variables aléatoires IID telles que : $Z_i \\in [-2,2]$ et $\\mathbb{E}[Z_i] = S$.\n",
    "On peut alors utiliser l'inégalité de Hoeffding pour borner la p-valeur : cette inégalité entraîne que, pour des variables IID $X_{i,...,n}$ bornées dans un intervalle $[A,B]$ :\n",
    "$$\\mathbb{P}\\left(|\\frac{1}{n}\\sum X_i - \\mathbb{E}[X] |> t\\right) \\leqslant 2\\exp\\left(\\frac{-nt^2}{(A-B)^2}\\right)$$\n",
    "\n",
    "Dans notre cas, la probabilité d'observer une statistique au moins aussi extrême que $\\hat S : |\\hat S| >2$, en supposant qu'on est dans une théorie à variables cachées locales, a la borne supérieure :\n",
    "$$pval = 2\\exp\\left(\\frac{-n(\\hat S - 2)^2}{4^2}\\right).$$\n",
    "\n",
    "L'inégalité CHSH est valable pour toutes les valeurs des angles $a1, b1, a2, b2$. En pratique, pour détecter une violation de CHSH dans une expérience, on pourra choisir des valeurs qu'on sait correspondre à un maximum de $|S|$, par exemple $a1 = 0°, a2 = 45°, b1 = 22.5°, b2 = 67.5°$ pour l'état EPR.  En revanche, si on ne connaît pas les angles ou l'état des paires de photons, on peut commencer par identifier la combinaison d'angles la plus prometteuse, en cherchant les angles qui maximisent $|\\hat{S}|$ sur une petite fraction des données. On peut alors faire le test pour ces angles-là en utilisant les données restantes (afin de ne pas invalider le test). (On pourra de nouveau utiliser la fonction `train_test_split` de `sklearn`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-schema",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123.75 78.75 11.25 146.25\n",
      "|S| max = 2.781900644533751, pvalue <= 1.1493492073266981e-48\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd # type: ignore\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_xx = pd.read_csv('datasets/jour2-1-facile/2photons-xx-1M.csv', sep=';')\n",
    "df_epr = pd.read_csv('datasets/jour2-1-facile/2photons-epr-1M.csv', sep=';')\n",
    "\n",
    "# Fonction de comptage des mesures et des coïncidences (XX, XY, ...)\n",
    "# ainsi que la quantité E, dans des tableaux 2D indexés par les angles\n",
    "# (alpha sur l'axe 0, beta sur l'axe 1).\n",
    "def count_detections(data, angles):\n",
    "    nb_angles = len(angles)\n",
    "    (count_measurements, count_xx, count_xy, count_yx, count_yy) = (\n",
    "        np.zeros((nb_angles, nb_angles)) for _ in range(5)\n",
    "    )\n",
    "    for i, alpha in enumerate(angles):\n",
    "        for j, beta in enumerate(angles):\n",
    "            select_angles = (data.alpha == alpha) & (data.beta == beta)\n",
    "            count_measurements[i, j] = select_angles.sum()\n",
    "            (Xa, Xb, Ya, Yb) = (data[name][select_angles] == 1 for name in ('Xa', 'Xb', 'Ya', 'Yb'))\n",
    "            count_xx[i, j] = (Xa & Xb).sum()\n",
    "            count_xy[i, j] = (Xa & Yb).sum()\n",
    "            count_yx[i, j] = (Ya & Xb).sum()\n",
    "            count_yy[i, j] = (Ya & Yb).sum()\n",
    "\n",
    "    E = (count_xx + count_yy - count_yx - count_xy) / count_measurements\n",
    "    return (count_measurements, count_xx, count_xy, count_yx, count_yy, E)\n",
    "\n",
    "# Fonction de calcul de S à partir du tableau E.\n",
    "def S_from_E(E):\n",
    "    n = E.shape[0]\n",
    "    assert E.shape == (n, n), f'Le tableau E doit être carré, pas de forme {E.shape}'\n",
    "    S = (E.reshape(n, 1, n, 1)   # alpha, beta\n",
    "         + E.reshape(1, n, n, 1) # alpha', beta\n",
    "         - E.reshape(n, 1, 1, n) # alpha, beta'\n",
    "         + E.reshape(1, n, 1, n) # alpha', beta'\n",
    "        )\n",
    "    return S\n",
    "\n",
    "# Pour le jeu de données testé, recherche des angles où |S| est max\n",
    "# sur une partie des données.\n",
    "data = df_epr\n",
    "angles = np.unique(data[['alpha', 'beta']])\n",
    "nb_angles = len(angles)\n",
    "data_search_S, data_test_S = train_test_split(data, train_size = 0.25)\n",
    "E = count_detections(data_search_S, angles)[-1]\n",
    "absS = np.abs(S_from_E(E))\n",
    "S_max = absS[np.isfinite(absS)].max() # Éviter les valeurs 0/0 avec isfinite().\n",
    "(i_alpha1, i_alpha2, i_beta1, i_beta2) = np.argwhere(absS == S_max)[0]\n",
    "(alpha1, alpha2, beta1, beta2) = (angles[i] for i in (i_alpha1, i_alpha2, i_beta1, i_beta2))\n",
    "print(alpha1, alpha2, beta1, beta2)\n",
    "\n",
    "# Calcul d'une borne de la p-valeur de |S| sur le reste des données\n",
    "# pour les angles trouvés où |S| était max ci-dessus.  Pour le nombre\n",
    "# de mesures, au cas où ce ne serait pas le même pour les différentes\n",
    "# combinaisons d'angles, vu qu'on calcule une borne, on peut prendre\n",
    "# le min.\n",
    "(counts, _, _, _, _, E) = count_detections(data_test_S, angles)\n",
    "S = E[i_alpha1, i_beta1] + E[i_alpha2, i_beta1] - E[i_alpha1, i_beta2] + E[i_alpha2, i_beta2]\n",
    "num = min(counts[i_alpha1, i_beta1], counts[i_alpha2, i_beta1],\n",
    "          counts[i_alpha1, i_beta2], counts[i_alpha2, i_beta2])\n",
    "pval = 2 * math.exp(-((abs(S) - 2)**2 * num) / 16)\n",
    "print(f'|S| max = {abs(S)}, pvalue <= {pval}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f14c6",
   "metadata": {},
   "source": [
    "### Influence du nombre de points de mesure\n",
    "\n",
    "Refaites le test avec les jeux de données de `datasets-jour3-supplement.zip` et commentez."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
